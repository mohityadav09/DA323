{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_mEv_hI44j4x"
      },
      "outputs": [],
      "source": [
        "categories = {\n",
        "    \"Fiction\": [\n",
        "        \"https://www.goodreads.com\",\n",
        "        \"https://www.tor.com\",\n",
        "        \"https://lithub.com\"\n",
        "    ],\n",
        "    \"Non-Fiction\": [\n",
        "        \"https://www.npr.org/sections/books\",\n",
        "        \"https://www.theguardian.com/books\",\n",
        "        \"https://www.newyorker.com/books\"\n",
        "    ],\n",
        "    \"Science Fiction & Fantasy\": [\n",
        "        \"https://www.tor.com\",\n",
        "        \"https://www.locusmag.com\",\n",
        "        \"https://www.sfsite.com\"\n",
        "    ],\n",
        "    \"Mystery & Thriller\": [\n",
        "        \"https://www.crimereads.com\",\n",
        "        \"https://www.mysteryscenemag.com\",\n",
        "        \"https://www.bookreporter.com\"\n",
        "    ],\n",
        "    \"Classic Literature\": [\n",
        "        \"https://www.gutenberg.org\",\n",
        "        \"https://www.britannica.com/art/classic-literature\",\n",
        "        \"https://www.openculture.com/free_ebooks\"\n",
        "    ],\n",
        "    \"Poetry\": [\n",
        "        \"https://www.poetryfoundation.org\",\n",
        "        \"https://www.poets.org\",\n",
        "        \"https://www.poetryarchive.org\"\n",
        "    ],\n",
        "    \"Children's Books\": [\n",
        "        \"https://www.scholastic.com\",\n",
        "        \"https://www.booktrust.org.uk\",\n",
        "        \"https://www.commonsensemedia.org\"\n",
        "    ],\n",
        "    \"Young Adult (YA)\": [\n",
        "        \"https://www.epicreads.com\",\n",
        "        \"https://www.teenreads.com\",\n",
        "        \"https://www.yalsa.ala.org/thehub/\"\n",
        "    ],\n",
        "    \"Historical Fiction\": [\n",
        "        \"https://historicalnovelsociety.org\",\n",
        "        \"https://www.bookbub.com/blog/tag/historical-fiction\",\n",
        "        \"https://www.nytimes.com/section/books\"\n",
        "    ],\n",
        "    \"Horror & Gothic\": [\n",
        "        \"https://www.the-line-up.com\",\n",
        "        \"https://www.horrorgeeklife.com\",\n",
        "        \"https://www.nightmare-magazine.com\"\n",
        "    ],\n",
        "    \"Graphic Novels & Comics\": [\n",
        "        \"https://www.comicbookherald.com\",\n",
        "        \"https://www.cbr.com\",\n",
        "        \"https://www.pastemagazine.com/comics\"\n",
        "    ],\n",
        "    \"Self-Help & Personal Development\": [\n",
        "        \"https://www.success.com\",\n",
        "        \"https://jamesclear.com\",\n",
        "        \"https://www.lifehack.org\"\n",
        "    ],\n",
        "    \"Book Reviews & Recommendations\": [\n",
        "        \"https://bookriot.com\",\n",
        "        \"https://www.kirkusreviews.com\",\n",
        "        \"https://www.nytimes.com/section/books/review\"\n",
        "    ],\n",
        "    \"Publishing & Writing\": [\n",
        "        \"https://www.writersdigest.com\",\n",
        "        \"https://www.thecreativepenn.com\",\n",
        "        \"https://www.nanowrimo.org\"\n",
        "    ],\n",
        "    \"Libraries & Archives\": [\n",
        "        \"https://www.loc.gov\",\n",
        "        \"https://www.worldcat.org\",\n",
        "        \"https://www.bl.uk\"\n",
        "    ]\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "from datetime import datetime\n",
        "\n",
        "# Create directory for output\n",
        "os.makedirs(\"scraped_data\", exist_ok=True)\n",
        "\n",
        "def extract_date(soup):\n",
        "    \"\"\"Extract article date from meta tags.\"\"\"\n",
        "    date_tags = [\"article:published_time\", \"date\", \"publish-date\", \"pubdate\"]\n",
        "    for tag in date_tags:\n",
        "        date_meta = soup.find(\"meta\", {\"property\": tag}) or soup.find(\"meta\", {\"name\": tag})\n",
        "        if date_meta and date_meta.get(\"content\"):\n",
        "            return date_meta[\"content\"]\n",
        "    return datetime.today().strftime(\"%Y-%m-%d\")  # Default to today's date\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"Cleans text by removing unnecessary spaces and line breaks.\"\"\"\n",
        "    text = re.sub(r\"\\s+\", \" \", text)  # Normalize whitespace\n",
        "    return text.strip()\n",
        "\n",
        "def scrape_and_save(category, urls):\n",
        "    structured_data = []\n",
        "\n",
        "    for url in urls:\n",
        "        try:\n",
        "            response = requests.get(url, headers={\"User-Agent\": \"Mozilla/5.0\"}, timeout=10)\n",
        "            response.raise_for_status()\n",
        "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "            title = soup.title.string.strip() if soup.title else \"Untitled Article\"\n",
        "            date = extract_date(soup)\n",
        "            paragraphs = [p.get_text(strip=True) for p in soup.find_all(\"p\")]\n",
        "            content = \"\\n\".join(paragraphs)\n",
        "\n",
        "            structured_data.append(f\"=== {title} ===\\nDate: {date}\\nSource: {url}\\n--------------------\\n{clean_text(content)}\\n\\n\")\n",
        "\n",
        "        except requests.RequestException as e:\n",
        "            print(f\"Error scraping {url}: {e}\")\n",
        "\n",
        "    file_path = os.path.join(\"scraped_data\", f\"{category}111.txt\")\n",
        "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.writelines(structured_data)\n",
        "\n",
        "    print(f\"Structured data saved for category: {category}\")\n",
        "\n",
        "\n",
        "\n",
        "# Scrape data\n",
        "for category, urls in categories.items():\n",
        "    scrape_and_save(category, urls)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q0TeO3iH41cm",
        "outputId": "0f628fbf-24ee-4546-886a-2c37a535f531"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Structured data saved for category: Fiction\n",
            "Structured data saved for category: Non-Fiction\n",
            "Error scraping https://www.sfsite.com: 404 Client Error: Not Found for url: https://www.sfsite.com/\n",
            "Structured data saved for category: Science Fiction & Fantasy\n",
            "Error scraping https://www.mysteryscenemag.com: HTTPSConnectionPool(host='www.mysteryscenemag.com', port=443): Max retries exceeded with url: / (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1006)')))\n",
            "Structured data saved for category: Mystery & Thriller\n",
            "Error scraping https://www.britannica.com/art/classic-literature: 404 Client Error: Not Found for url: https://www.britannica.com/art/classic-literature\n",
            "Structured data saved for category: Classic Literature\n",
            "Structured data saved for category: Poetry\n",
            "Structured data saved for category: Children's Books\n",
            "Error scraping https://www.epicreads.com: 403 Client Error: Forbidden for url: https://www.epicreads.com/\n",
            "Structured data saved for category: Young Adult (YA)\n",
            "Error scraping https://www.bookbub.com/blog/tag/historical-fiction: 403 Client Error: Forbidden for url: https://www.bookbub.com/blog/tag/historical-fiction\n",
            "Structured data saved for category: Historical Fiction\n",
            "Structured data saved for category: Horror & Gothic\n",
            "Error scraping https://www.cbr.com: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
            "Error scraping https://www.pastemagazine.com/comics: 403 Client Error: Forbidden for url: https://www.pastemagazine.com/comics\n",
            "Structured data saved for category: Graphic Novels & Comics\n",
            "Structured data saved for category: Self-Help & Personal Development\n",
            "Structured data saved for category: Book Reviews & Recommendations\n",
            "Error scraping https://www.writersdigest.com: 403 Client Error: Forbidden for url: https://www.writersdigest.com/\n",
            "Structured data saved for category: Publishing & Writing\n",
            "Error scraping https://www.worldcat.org: 403 Client Error: Forbidden for url: https://search.worldcat.org/\n",
            "Structured data saved for category: Libraries & Archives\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filepath = \"scraped_data/Fiction111.txt\"\n",
        "with open(filepath, \"r\", encoding=\"utf-8\") as file:\n",
        "    content = file.read()\n",
        "\n",
        "print(content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RZkTGnE744i2",
        "outputId": "347cdec6-6cac-4c39-d4c4-77d23a2ba247"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Goodreads | Meet your next favorite book ===\n",
            "Date: 2025-03-09\n",
            "Source: https://www.goodreads.com\n",
            "--------------------\n",
            "You’re in the right place. Tell us what titles or genres you’ve enjoyed in the past, and we’ll give you surprisingly insightful recommendations. Chances are your friends are discussing their favorite (and least favorite) books on Goodreads. Because Brian liked… He discovered: Decision-making, Sociology, Marketing Because Shomeret liked… She discovered: Psychology, Animals, Science, Nature More book lists Gain access to a massive audience of book lovers. Goodreads is a great place to promote your books. Welcome back. Just a moment while we sign you in to your Goodreads account.\n",
            "\n",
            "=== Homepage - Reactor ===\n",
            "Date: 2025-03-09\n",
            "Source: https://www.tor.com\n",
            "--------------------\n",
            "Advertisement Science fiction. Fantasy. The universe. And related subjects. The Reactor newsletter is the best way to catch up on the world of science fiction, fantasy, pop culture, and more! Kochin is a heartsooth—a rare being with the ability to heal any wound. Any wound, that is, exce... Werewolves, queens, and assassins are just some of the interesting folks you'll encounter in Mar... Hedgewitch apprentices, engineers, students, and paranormal investigators all appear in March's... A queer standalone YA fantasy full of pirates and sea-magic—and one terrifying monster. The world of teen horror is a scary place, especially in tales of isolation... Works on Ancient Greece to the Inquisition and the Renaissance, WWII to the present day… When Colin meets a shadowy figure promising him his heart’s deepest desire, he can’t resist... Lanie Stones is the necromancer that Death has been praying for… In this tender, eerie, and hard-to-classify novel, two sisters live in the Garden, isolated from... France is under water, Korea is reunited, and a meteorite has struck Washington, D.C. in March's... The roots of this one go all the way back to Revelation... When does a genre literary trope cross the threshold into marketing trend? A woman about to leave on an overseas business trip, calls home from the airport and discovers t... A new story set in the world of “The Red Mother.” Hacksilver riddled with a dragon, saved his fa... A pair of sisters are hired to find--and if necessary, dispose of--whatever is killing neighborh... The spirit of a recently deceased young boy helps a group of ghosts seek revenge on a corrupt an... Mel relishes running the “Enchanted Jungle,” a roadside attraction in the Everglades filled with... Teenage Graff dreams of going off-world to explore the universe as a documentarian, but he never... A woman losing her sight turns to small family magics to save the lives of those she loves the m... In one small town, the delicate balance between predator and prey is threatened when five girls... The Reactor newsletter is the best way to catch up on the world of science fiction, fantasy, pop culture, and more! “The sky above the port was the color of television, tuned to a dead channel.” William Gibson, Neuromancer TPG 2025•Site Credit For compliance with applicable privacy laws:\n",
            "\n",
            "=== Literary  Hub ===\n",
            "Date: 2025-03-09\n",
            "Source: https://lithub.com\n",
            "--------------------\n",
            "Featuring Brad Johnson, Emily Temple, James Folta, and Drew Broussard In Praise of an Early Poetic Chronicler of Black Queer Life Rebecca Worby on Will Stratton’s “Points of Origin” Featuring Chimamanda Ngozi Adichie, Laila Lalami, Chloe Dalton, and More Jennifer Lucy Allan on the Millennia-Long Relationship Between Humans and Hands-On Creation Nick Thorpe Explores the Intersections of Geography and Culture in Central and Eastern Europe Remembering an Award-Winning Writer Who Sacrificed Her Life For Justice Created by Grove Atlantic and Electric Literature Masthead About Sign Up For Our Newsletters How to Pitch Lit Hub Advertisers: Contact Us Privacy Policy Support Lit Hub - Become A Member\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "\n",
        "# Create directory for output\n",
        "os.makedirs(\"scraped_data\", exist_ok=True)\n",
        "\n",
        "def extract_date(soup):\n",
        "    \"\"\"Extract article date from meta tags.\"\"\"\n",
        "    date_tags = [\"article:published_time\", \"date\", \"publish-date\", \"pubdate\"]\n",
        "    for tag in date_tags:\n",
        "        date_meta = soup.find(\"meta\", {\"property\": tag}) or soup.find(\"meta\", {\"name\": tag})\n",
        "        if date_meta and date_meta.get(\"content\"):\n",
        "            return date_meta[\"content\"]\n",
        "    return datetime.today().strftime(\"%Y-%m-%d\")  # Default to today's date\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"Cleans text by removing unnecessary spaces and line breaks.\"\"\"\n",
        "    text = re.sub(r\"\\s+\", \" \", text)  # Normalize whitespace\n",
        "    return text.strip()\n",
        "\n",
        "def scrape_and_save(category, urls, all_data):\n",
        "    for url in urls:\n",
        "        try:\n",
        "            response = requests.get(url, headers={\"User-Agent\": \"Mozilla/5.0\"}, timeout=10)\n",
        "            response.raise_for_status()\n",
        "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "            title = soup.title.string.strip() if soup.title else \"Untitled Article\"\n",
        "            date = extract_date(soup)\n",
        "            paragraphs = [p.get_text(strip=True) for p in soup.find_all(\"p\")]\n",
        "            content = clean_text(\" \".join(paragraphs))\n",
        "\n",
        "            # Append data to list\n",
        "            all_data.append([category, title, date, url, content])\n",
        "\n",
        "        except requests.RequestException as e:\n",
        "            print(f\"Error scraping {url}: {e}\")\n",
        "\n",
        "# Collect data\n",
        "all_data = []\n",
        "for category, urls in categories.items():\n",
        "    scrape_and_save(category, urls, all_data)\n",
        "\n",
        "# Convert to DataFrame\n",
        "df = pd.DataFrame(all_data, columns=[\"Category\", \"Title\", \"Date\", \"URL\", \"Content\"])\n",
        "\n",
        "# Save to CSV\n",
        "csv_path = os.path.join(\"scraped_data\", \"scraped_articles.csv\")\n",
        "df.to_csv(csv_path, index=False, encoding=\"utf-8\")\n",
        "\n",
        "print(f\"Data saved successfully to {csv_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NB76_PRb5-j3",
        "outputId": "4d12f25e-a0f6-4a22-b8bd-34dff637d909"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error scraping https://www.sfsite.com: 404 Client Error: Not Found for url: https://www.sfsite.com/\n",
            "Error scraping https://www.mysteryscenemag.com: HTTPSConnectionPool(host='www.mysteryscenemag.com', port=443): Max retries exceeded with url: / (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1006)')))\n",
            "Error scraping https://www.britannica.com/art/classic-literature: 404 Client Error: Not Found for url: https://www.britannica.com/art/classic-literature\n",
            "Error scraping https://www.epicreads.com: 403 Client Error: Forbidden for url: https://www.epicreads.com/\n",
            "Error scraping https://www.bookbub.com/blog/tag/historical-fiction: 403 Client Error: Forbidden for url: https://www.bookbub.com/blog/tag/historical-fiction\n",
            "Error scraping https://www.cbr.com: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
            "Error scraping https://www.pastemagazine.com/comics: 403 Client Error: Forbidden for url: https://www.pastemagazine.com/comics\n",
            "Error scraping https://www.writersdigest.com: 403 Client Error: Forbidden for url: https://www.writersdigest.com/\n",
            "Error scraping https://www.worldcat.org: 403 Client Error: Forbidden for url: https://search.worldcat.org/\n",
            "Data saved successfully to scraped_data/scraped_articles.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('/content/scraped_data/scraped_articles.csv')\n",
        "data.head()\n",
        "data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ooluM1L96mG5",
        "outputId": "571a3445-89f4-4139-e619-c8af7174c03e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(36, 5)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.dropna(inplace=True)\n",
        "data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_KwyMVRA7aU4",
        "outputId": "e022b4b7-cc25-4f1c-b627-e9864b5e364f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(33, 5)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "pokYlajB7j7x",
        "outputId": "4fcd4a20-2b45-4775-9b21-d3f1306d51fb"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      Category                                              Title        Date  \\\n",
              "0      Fiction           Goodreads | Meet your next favorite book  2025-03-09   \n",
              "1      Fiction                                 Homepage - Reactor  2025-03-09   \n",
              "2      Fiction                                      Literary  Hub  2025-03-09   \n",
              "3  Non-Fiction  Books: Book Reviews, Book News, and Author Int...  2025-03-09   \n",
              "5  Non-Fiction  Culture: TV, Movies, Music, Art, and Theatre N...  2025-03-09   \n",
              "\n",
              "                                  URL  \\\n",
              "0           https://www.goodreads.com   \n",
              "1                 https://www.tor.com   \n",
              "2                  https://lithub.com   \n",
              "3  https://www.npr.org/sections/books   \n",
              "5     https://www.newyorker.com/books   \n",
              "\n",
              "                                             Content  \n",
              "0  You’re in the right place. Tell us what titles...  \n",
              "1  Advertisement Science fiction. Fantasy. The un...  \n",
              "2  Featuring Brad Johnson, Emily Temple, James Fo...  \n",
              "3  November 25, 2024 •Books We Love returns with ...  \n",
              "5  Sections More ©2025Condé Nast. All rights rese...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-bff7652f-c73e-4301-a781-3824ff3b83f2\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Category</th>\n",
              "      <th>Title</th>\n",
              "      <th>Date</th>\n",
              "      <th>URL</th>\n",
              "      <th>Content</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Fiction</td>\n",
              "      <td>Goodreads | Meet your next favorite book</td>\n",
              "      <td>2025-03-09</td>\n",
              "      <td>https://www.goodreads.com</td>\n",
              "      <td>You’re in the right place. Tell us what titles...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Fiction</td>\n",
              "      <td>Homepage - Reactor</td>\n",
              "      <td>2025-03-09</td>\n",
              "      <td>https://www.tor.com</td>\n",
              "      <td>Advertisement Science fiction. Fantasy. The un...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Fiction</td>\n",
              "      <td>Literary  Hub</td>\n",
              "      <td>2025-03-09</td>\n",
              "      <td>https://lithub.com</td>\n",
              "      <td>Featuring Brad Johnson, Emily Temple, James Fo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Non-Fiction</td>\n",
              "      <td>Books: Book Reviews, Book News, and Author Int...</td>\n",
              "      <td>2025-03-09</td>\n",
              "      <td>https://www.npr.org/sections/books</td>\n",
              "      <td>November 25, 2024 •Books We Love returns with ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Non-Fiction</td>\n",
              "      <td>Culture: TV, Movies, Music, Art, and Theatre N...</td>\n",
              "      <td>2025-03-09</td>\n",
              "      <td>https://www.newyorker.com/books</td>\n",
              "      <td>Sections More ©2025Condé Nast. All rights rese...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-bff7652f-c73e-4301-a781-3824ff3b83f2')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-bff7652f-c73e-4301-a781-3824ff3b83f2 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-bff7652f-c73e-4301-a781-3824ff3b83f2');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-f8d1b420-12e0-44ea-9239-287e68de552f\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f8d1b420-12e0-44ea-9239-287e68de552f')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-f8d1b420-12e0-44ea-9239-287e68de552f button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "data",
              "summary": "{\n  \"name\": \"data\",\n  \"rows\": 33,\n  \"fields\": [\n    {\n      \"column\": \"Category\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 15,\n        \"samples\": [\n          \"Horror & Gothic\",\n          \"Self-Help & Personal Development\",\n          \"Fiction\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 31,\n        \"samples\": [\n          \"Kirkus Reviews\",\n          \"Common Sense Media: Age-Based Media Reviews for Families | Common Sense Media\",\n          \"SUCCESS | Your Trusted Guide to the Future of Work\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Date\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"2025-03-09\",\n          \"2012-04-25T14:18:00.000Z\",\n          \"2012-06-29T19:41:07+00:00\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"URL\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 32,\n        \"samples\": [\n          \"https://www.thecreativepenn.com\",\n          \"https://www.commonsensemedia.org\",\n          \"https://jamesclear.com\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Content\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 31,\n        \"samples\": [\n          \"Browse by Genre Popular Content Popular Genres Browse by Content Type Winners & Finalists General Information Pre-publication book reviews and features keeping readers and industry influencers in the know since 1933. Resources & Education Services for Authors Kirkus Diversity Collections Kirkus Pro Connect NONFICTION FICTION NONFICTION SCIENCE FICTION & FANTASY NONFICTION NONFICTION FICTION ROMANCE NONFICTION NONFICTION FICTION FICTION SCIENCE FICTION & FANTASY FICTION NONFICTION NONFICTION NONFICTION NONFICTION NONFICTION NONFICTION HOT THIS WEEK HOT THIS WEEK FIND YOUR NEW FAVORITE BOOK TOP STORY HOT THIS WEEK FIND YOUR NEW FAVORITE BOOK START HERE SELECT TOP STORY TOP STORY Our Verdict Our Verdict Our Verdict Our Verdict Our Verdict Our Verdict Our Verdict Our Verdict Our Verdict Our Verdict Our Verdict Our Verdict Our Verdict Our Verdict Our Verdict FULLY BOOKED PODCAST March 4, 2025 Laila Lalami discusses \\u2018The Dream Hotel\\u2019 on our Best March Books episode. LISTEN AND SUBSCRIBE ONSPOTIFY,APPLE,AMAZON MUSIC, ORPODCASTONE MORE EPISODES Feb. 25, 2025 The personal is cyclical: Lidia Yuknavitch makes a triumphant return to memoir. Feb. 18, 2025 Novelist Erin Crosby Eckstine joins us for a special episode celebrating debuts. Feb. 11, 2025 Paul Lisicky contemplates the profound influence of Joni Mitchell on his life and work. INSIDE SCOOP INSIDE SCOOP The Magazine:Kirkus Reviews Featuring 302 industry-first reviews of fiction, nonfiction, children\\u2019s, and YA books; also in this issue: interviews with Jojo Moyes, John Green, Saadia Faruqi, and Ashley Hope P\\u00e9rez; and more The Kirkus Star One of the most coveted designations in the book industry, the Kirkus Star marks books of exceptional merit. The Kirkus Prize The Kirkus Prize is among the richest literary awards in America, awarding $50,000 in three categories annually. Be the first to read books news and see reviews, news and features inKirkus Reviews. Get awesome content delivered to your inbox every week. \\u00a9 Copyright 2025 Kirkus Media LLC.All Rights Reserved. Follow Popular in this Genre Hey there, book lover. We\\u2019re glad you found a book that interests you! OR We can\\u2019t wait for you to join Kirkus! It\\u2019s free and takes less than 10 seconds! Already have an account?Log in. OR Trouble signing in?Retrieve credentials. Already have an account?Log in. Welcome Back! OR Trouble signing in?Retrieve credentials. Don\\u2019t fret. We\\u2019ll find you. All Users Magazine Subscribers (How to Find Your Reader Number) If You\\u2019ve Purchased Author Services Don\\u2019t have an account yet?Sign Up.\",\n          \"Common Sense Media Movie & TV reviews for parents Droll, very mature look at sex worker's whirlwind romance. Raucous, violent, darkly comic sci-fi movie about humanity. Lots of language, drugs in \\\"Ted Lasso\\\"-ish sports comedy. Hero returns in gripping superhero saga; violence, language. Lots of language, drugs in \\\"Ted Lasso\\\"-ish sports comedy. Hero returns in gripping superhero saga; violence, language. Entertaining, inspiring films that show the value of perseverance and overcoming obstacles. Help kids learn the value of empathy, compassion, and treating others with respect and caring. Read our reviews of the books TikTok influencers love. 71 parent reviews 12 parent reviews 8 parent reviews 23 parent reviews Since 2003, Common Sense has been the leading independent source for media recommendations and advice for families. Look for us next time you're searching for something to watch, read, or play. Not-for-profit partnerships, generous foundation support, and contributions from parents like you keep Common Sense free and available to families everywhere. Common Sense is dedicated to improving the lives of kids and families by providing the trustworthy information, education, and independent voice they need to thrive. We're a nonprofit.Support our work\",\n          \"Save Up to 20% Before Oct. 2! byJeet Kumar Ambasth bySusan B. Barnes byMegan Marshall 1 2 3 Check out the full list of SUCCESS\\u00aeBestsellers in business,personal development, finance, leadership, and more. Dive deep into the heart of our brand with the latest trends and entertainment, sneak peeks into upcoming projects, and exclusive insights that only our email subscribers get to enjoy. Get exclusive access to world-class speakersthrough the SUCCESS Speakers Bureau. 5473 Blair Road, Suite 100PMB 30053Dallas, TX 75231 Copyright \\u00a9 2025 SUCCESS Magazine. All rights reserved. Save Up to 20% Before Oct. 2! Copyright \\u00a9 2024 SUCCESS Magazine. All rights reserved. Unlock the Latest Knowledge that Can You Help You Achieve More in Life with More Confidence Print and Digital Options Available You\\u2019vereached your limitoffreearticles for this month! (plus get access to hundreds of resources designedto help you excel in life and business) Please enter your username or email address. You will receive an email message to log in. No, thanks,I\\u2019m not interested in personal growth. Plus, get access to daily inspiration, weekly newsletters and podcasts, and occasional updates from us. By signing up you are also added to SUCCESS\\u00ae emails. You can easily unsubscribe at anytime. By clicking above, you agree to our Privacy Policy and Terms of Use. Please enter your username or email address. You will receive an email message to log in. Get unlimited access to SUCCESS\\u00ae(+ a bunch of extras)! Learn more. Please enter your username or email address. You will receive an email message to log in. The exclusive article you\\u2019re trying to view is for subscribers only. (plus get access to hundreds of resources designedto help you excel in life and business) Please enter your username or email address. You will receive an email message to log in.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Load your dataset (ensure this is defined)\n",
        "# data = pd.read_csv(\"scraped_articles.csv\")  # Uncomment if reading from a CSV\n",
        "\n",
        "# Preprocess the text data\n",
        "def preprocess_text(text):\n",
        "    \"\"\"Convert text to lowercase, remove punctuation, and split into words.\"\"\"\n",
        "    text = text.lower()\n",
        "    text = ''.join(c for c in text if c.isalnum() or c.isspace())\n",
        "    return text.split()\n",
        "\n",
        "data['Processed_Content'] = data['Content'].apply(preprocess_text)\n",
        "\n",
        "# Train a Word2Vec model\n",
        "sentences = data['Processed_Content'].tolist()\n",
        "word2vec_model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "# Save trained embeddings (optional)\n",
        "word2vec_model.save(\"word2vec_model.bin\")\n",
        "\n",
        "# Generate document embeddings using trained Word2Vec embeddings\n",
        "def document_embedding(words, model):\n",
        "    \"\"\"Get average Word2Vec embedding for a document.\"\"\"\n",
        "    embeddings = [model.wv.get_vector(word) for word in words if word in model.wv]\n",
        "    if embeddings:\n",
        "        return np.mean(embeddings, axis=0)\n",
        "    else:\n",
        "        return np.zeros(model.vector_size)  # Default zero vector if no words found\n",
        "\n",
        "# Convert text to embeddings\n",
        "data['Content_Embedding'] = data['Processed_Content'].apply(lambda x: document_embedding(x, word2vec_model))\n",
        "\n",
        "# Convert list of embeddings into a DataFrame\n",
        "X = pd.DataFrame(list(data['Content_Embedding']))\n",
        "y = data['Category']\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a Random Forest classifier\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = rf_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NPgNhCKr74WI",
        "outputId": "4cc02c92-ad19-4252-e8b4-af7fc756ee81"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                  precision    recall  f1-score   support\n",
            "\n",
            "                Children's Books       0.00      0.00      0.00       1.0\n",
            "              Classic Literature       0.00      0.00      0.00       1.0\n",
            "                         Fiction       0.00      0.00      0.00       0.0\n",
            "              Historical Fiction       0.00      0.00      0.00       1.0\n",
            "                 Horror & Gothic       0.00      0.00      0.00       0.0\n",
            "            Libraries & Archives       0.00      0.00      0.00       1.0\n",
            "              Mystery & Thriller       0.00      0.00      0.00       1.0\n",
            "                          Poetry       0.00      0.00      0.00       0.0\n",
            "       Science Fiction & Fantasy       0.00      0.00      0.00       0.0\n",
            "Self-Help & Personal Development       0.00      0.00      0.00       1.0\n",
            "                Young Adult (YA)       0.00      0.00      0.00       1.0\n",
            "\n",
            "                        accuracy                           0.00       7.0\n",
            "                       macro avg       0.00      0.00      0.00       7.0\n",
            "                    weighted avg       0.00      0.00      0.00       7.0\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tq9AGW618S3n",
        "outputId": "001f6558-ccf5-43ac-8542-f136de4e343e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sentence_transformers import SentenceTransformer"
      ],
      "metadata": {
        "id": "4x8FIN_y8OPQ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = data.dropna(subset=['Content', 'Category'])\n",
        "\n",
        "# Load Hugging Face embedding model\n",
        "embed_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "# Generate embeddings\n",
        "data['Content_Embedding'] = data['Content'].apply(lambda x: embed_model.encode(x, convert_to_tensor=True))"
      ],
      "metadata": {
        "id": "DJzOSQPz85rL"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode categories\n",
        "label_encoder = LabelEncoder()\n",
        "data['Category_Label'] = label_encoder.fit_transform(data['Category'])\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    data['Content_Embedding'].tolist(), data['Category_Label'], test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Convert lists to tensors\n",
        "X_train, X_test = torch.stack(X_train), torch.stack(X_test)\n",
        "\n",
        "# Convert y_train and y_test from pandas Series to NumPy arrays first, then to tensors\n",
        "y_train, y_test = torch.tensor(y_train.to_numpy(), dtype=torch.long), torch.tensor(y_test.to_numpy(), dtype=torch.long)\n",
        "\n",
        "# Create a PyTorch dataset\n",
        "class NewsDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "train_dataset = NewsDataset(X_train, y_train)\n",
        "test_dataset = NewsDataset(X_test, y_test)\n",
        "\n",
        "# Dataloaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32)\n"
      ],
      "metadata": {
        "id": "_vjhTjM889Yh"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a simple Neural Network\n",
        "class TextClassifier(nn.Module):\n",
        "    def __init__(self, input_dim, num_classes):\n",
        "        super(TextClassifier, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 256)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(256, num_classes)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return self.softmax(x)\n",
        "\n",
        "# Model setup\n",
        "num_classes = len(label_encoder.classes_)\n",
        "model = TextClassifier(input_dim=X_train.shape[1], num_classes=num_classes)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 500\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(X_batch)\n",
        "        loss = criterion(outputs, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/len(train_loader):.4f}\")\n",
        "\n",
        "# Evaluate the model\n",
        "model.eval()\n",
        "correct, total = 0, 0\n",
        "with torch.no_grad():\n",
        "    for X_batch, y_batch in test_loader:\n",
        "        outputs = model(X_batch)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        correct += (predicted == y_batch).sum().item()\n",
        "        total += y_batch.size(0)\n",
        "\n",
        "print(f\"Accuracy: {correct / total:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nx7hBvLt9EL9",
        "outputId": "38347cf1-eecb-44ab-c05c-cfc8d632f073"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/500], Loss: 2.7089\n",
            "Epoch [2/500], Loss: 2.7086\n",
            "Epoch [3/500], Loss: 2.7084\n",
            "Epoch [4/500], Loss: 2.7081\n",
            "Epoch [5/500], Loss: 2.7078\n",
            "Epoch [6/500], Loss: 2.7074\n",
            "Epoch [7/500], Loss: 2.7069\n",
            "Epoch [8/500], Loss: 2.7064\n",
            "Epoch [9/500], Loss: 2.7057\n",
            "Epoch [10/500], Loss: 2.7050\n",
            "Epoch [11/500], Loss: 2.7040\n",
            "Epoch [12/500], Loss: 2.7027\n",
            "Epoch [13/500], Loss: 2.7011\n",
            "Epoch [14/500], Loss: 2.6990\n",
            "Epoch [15/500], Loss: 2.6962\n",
            "Epoch [16/500], Loss: 2.6926\n",
            "Epoch [17/500], Loss: 2.6878\n",
            "Epoch [18/500], Loss: 2.6818\n",
            "Epoch [19/500], Loss: 2.6740\n",
            "Epoch [20/500], Loss: 2.6639\n",
            "Epoch [21/500], Loss: 2.6511\n",
            "Epoch [22/500], Loss: 2.6348\n",
            "Epoch [23/500], Loss: 2.6149\n",
            "Epoch [24/500], Loss: 2.5922\n",
            "Epoch [25/500], Loss: 2.5689\n",
            "Epoch [26/500], Loss: 2.5467\n",
            "Epoch [27/500], Loss: 2.5255\n",
            "Epoch [28/500], Loss: 2.5032\n",
            "Epoch [29/500], Loss: 2.4785\n",
            "Epoch [30/500], Loss: 2.4506\n",
            "Epoch [31/500], Loss: 2.4236\n",
            "Epoch [32/500], Loss: 2.4038\n",
            "Epoch [33/500], Loss: 2.3910\n",
            "Epoch [34/500], Loss: 2.3812\n",
            "Epoch [35/500], Loss: 2.3720\n",
            "Epoch [36/500], Loss: 2.3615\n",
            "Epoch [37/500], Loss: 2.3483\n",
            "Epoch [38/500], Loss: 2.3378\n",
            "Epoch [39/500], Loss: 2.3383\n",
            "Epoch [40/500], Loss: 2.3311\n",
            "Epoch [41/500], Loss: 2.3191\n",
            "Epoch [42/500], Loss: 2.3132\n",
            "Epoch [43/500], Loss: 2.3092\n",
            "Epoch [44/500], Loss: 2.3039\n",
            "Epoch [45/500], Loss: 2.2979\n",
            "Epoch [46/500], Loss: 2.2911\n",
            "Epoch [47/500], Loss: 2.2798\n",
            "Epoch [48/500], Loss: 2.2613\n",
            "Epoch [49/500], Loss: 2.2411\n",
            "Epoch [50/500], Loss: 2.2429\n",
            "Epoch [51/500], Loss: 2.2408\n",
            "Epoch [52/500], Loss: 2.2316\n",
            "Epoch [53/500], Loss: 2.2248\n",
            "Epoch [54/500], Loss: 2.2219\n",
            "Epoch [55/500], Loss: 2.2202\n",
            "Epoch [56/500], Loss: 2.2171\n",
            "Epoch [57/500], Loss: 2.2123\n",
            "Epoch [58/500], Loss: 2.2061\n",
            "Epoch [59/500], Loss: 2.1998\n",
            "Epoch [60/500], Loss: 2.1928\n",
            "Epoch [61/500], Loss: 2.1821\n",
            "Epoch [62/500], Loss: 2.1708\n",
            "Epoch [63/500], Loss: 2.1678\n",
            "Epoch [64/500], Loss: 2.1552\n",
            "Epoch [65/500], Loss: 2.1435\n",
            "Epoch [66/500], Loss: 2.1360\n",
            "Epoch [67/500], Loss: 2.1279\n",
            "Epoch [68/500], Loss: 2.1190\n",
            "Epoch [69/500], Loss: 2.1105\n",
            "Epoch [70/500], Loss: 2.1037\n",
            "Epoch [71/500], Loss: 2.0989\n",
            "Epoch [72/500], Loss: 2.0945\n",
            "Epoch [73/500], Loss: 2.0912\n",
            "Epoch [74/500], Loss: 2.0892\n",
            "Epoch [75/500], Loss: 2.0874\n",
            "Epoch [76/500], Loss: 2.0853\n",
            "Epoch [77/500], Loss: 2.0839\n",
            "Epoch [78/500], Loss: 2.0831\n",
            "Epoch [79/500], Loss: 2.0824\n",
            "Epoch [80/500], Loss: 2.0819\n",
            "Epoch [81/500], Loss: 2.0814\n",
            "Epoch [82/500], Loss: 2.0811\n",
            "Epoch [83/500], Loss: 2.0811\n",
            "Epoch [84/500], Loss: 2.0811\n",
            "Epoch [85/500], Loss: 2.0809\n",
            "Epoch [86/500], Loss: 2.0806\n",
            "Epoch [87/500], Loss: 2.0805\n",
            "Epoch [88/500], Loss: 2.0804\n",
            "Epoch [89/500], Loss: 2.0804\n",
            "Epoch [90/500], Loss: 2.0803\n",
            "Epoch [91/500], Loss: 2.0803\n",
            "Epoch [92/500], Loss: 2.0802\n",
            "Epoch [93/500], Loss: 2.0800\n",
            "Epoch [94/500], Loss: 2.0798\n",
            "Epoch [95/500], Loss: 2.0797\n",
            "Epoch [96/500], Loss: 2.0797\n",
            "Epoch [97/500], Loss: 2.0797\n",
            "Epoch [98/500], Loss: 2.0796\n",
            "Epoch [99/500], Loss: 2.0795\n",
            "Epoch [100/500], Loss: 2.0795\n",
            "Epoch [101/500], Loss: 2.0795\n",
            "Epoch [102/500], Loss: 2.0795\n",
            "Epoch [103/500], Loss: 2.0795\n",
            "Epoch [104/500], Loss: 2.0794\n",
            "Epoch [105/500], Loss: 2.0794\n",
            "Epoch [106/500], Loss: 2.0794\n",
            "Epoch [107/500], Loss: 2.0793\n",
            "Epoch [108/500], Loss: 2.0793\n",
            "Epoch [109/500], Loss: 2.0793\n",
            "Epoch [110/500], Loss: 2.0792\n",
            "Epoch [111/500], Loss: 2.0792\n",
            "Epoch [112/500], Loss: 2.0792\n",
            "Epoch [113/500], Loss: 2.0792\n",
            "Epoch [114/500], Loss: 2.0792\n",
            "Epoch [115/500], Loss: 2.0792\n",
            "Epoch [116/500], Loss: 2.0792\n",
            "Epoch [117/500], Loss: 2.0792\n",
            "Epoch [118/500], Loss: 2.0792\n",
            "Epoch [119/500], Loss: 2.0792\n",
            "Epoch [120/500], Loss: 2.0792\n",
            "Epoch [121/500], Loss: 2.0792\n",
            "Epoch [122/500], Loss: 2.0792\n",
            "Epoch [123/500], Loss: 2.0792\n",
            "Epoch [124/500], Loss: 2.0792\n",
            "Epoch [125/500], Loss: 2.0792\n",
            "Epoch [126/500], Loss: 2.0792\n",
            "Epoch [127/500], Loss: 2.0792\n",
            "Epoch [128/500], Loss: 2.0792\n",
            "Epoch [129/500], Loss: 2.0792\n",
            "Epoch [130/500], Loss: 2.0792\n",
            "Epoch [131/500], Loss: 2.0792\n",
            "Epoch [132/500], Loss: 2.0792\n",
            "Epoch [133/500], Loss: 2.0792\n",
            "Epoch [134/500], Loss: 2.0792\n",
            "Epoch [135/500], Loss: 2.0792\n",
            "Epoch [136/500], Loss: 2.0792\n",
            "Epoch [137/500], Loss: 2.0792\n",
            "Epoch [138/500], Loss: 2.0792\n",
            "Epoch [139/500], Loss: 2.0792\n",
            "Epoch [140/500], Loss: 2.0792\n",
            "Epoch [141/500], Loss: 2.0792\n",
            "Epoch [142/500], Loss: 2.0792\n",
            "Epoch [143/500], Loss: 2.0792\n",
            "Epoch [144/500], Loss: 2.0792\n",
            "Epoch [145/500], Loss: 2.0792\n",
            "Epoch [146/500], Loss: 2.0792\n",
            "Epoch [147/500], Loss: 2.0792\n",
            "Epoch [148/500], Loss: 2.0792\n",
            "Epoch [149/500], Loss: 2.0792\n",
            "Epoch [150/500], Loss: 2.0792\n",
            "Epoch [151/500], Loss: 2.0792\n",
            "Epoch [152/500], Loss: 2.0792\n",
            "Epoch [153/500], Loss: 2.0792\n",
            "Epoch [154/500], Loss: 2.0792\n",
            "Epoch [155/500], Loss: 2.0792\n",
            "Epoch [156/500], Loss: 2.0792\n",
            "Epoch [157/500], Loss: 2.0791\n",
            "Epoch [158/500], Loss: 2.0791\n",
            "Epoch [159/500], Loss: 2.0791\n",
            "Epoch [160/500], Loss: 2.0791\n",
            "Epoch [161/500], Loss: 2.0791\n",
            "Epoch [162/500], Loss: 2.0791\n",
            "Epoch [163/500], Loss: 2.0791\n",
            "Epoch [164/500], Loss: 2.0791\n",
            "Epoch [165/500], Loss: 2.0791\n",
            "Epoch [166/500], Loss: 2.0791\n",
            "Epoch [167/500], Loss: 2.0791\n",
            "Epoch [168/500], Loss: 2.0791\n",
            "Epoch [169/500], Loss: 2.0791\n",
            "Epoch [170/500], Loss: 2.0791\n",
            "Epoch [171/500], Loss: 2.0791\n",
            "Epoch [172/500], Loss: 2.0791\n",
            "Epoch [173/500], Loss: 2.0791\n",
            "Epoch [174/500], Loss: 2.0791\n",
            "Epoch [175/500], Loss: 2.0791\n",
            "Epoch [176/500], Loss: 2.0791\n",
            "Epoch [177/500], Loss: 2.0791\n",
            "Epoch [178/500], Loss: 2.0791\n",
            "Epoch [179/500], Loss: 2.0791\n",
            "Epoch [180/500], Loss: 2.0791\n",
            "Epoch [181/500], Loss: 2.0791\n",
            "Epoch [182/500], Loss: 2.0791\n",
            "Epoch [183/500], Loss: 2.0791\n",
            "Epoch [184/500], Loss: 2.0791\n",
            "Epoch [185/500], Loss: 2.0791\n",
            "Epoch [186/500], Loss: 2.0791\n",
            "Epoch [187/500], Loss: 2.0791\n",
            "Epoch [188/500], Loss: 2.0791\n",
            "Epoch [189/500], Loss: 2.0791\n",
            "Epoch [190/500], Loss: 2.0791\n",
            "Epoch [191/500], Loss: 2.0791\n",
            "Epoch [192/500], Loss: 2.0791\n",
            "Epoch [193/500], Loss: 2.0791\n",
            "Epoch [194/500], Loss: 2.0791\n",
            "Epoch [195/500], Loss: 2.0791\n",
            "Epoch [196/500], Loss: 2.0791\n",
            "Epoch [197/500], Loss: 2.0791\n",
            "Epoch [198/500], Loss: 2.0791\n",
            "Epoch [199/500], Loss: 2.0791\n",
            "Epoch [200/500], Loss: 2.0791\n",
            "Epoch [201/500], Loss: 2.0791\n",
            "Epoch [202/500], Loss: 2.0791\n",
            "Epoch [203/500], Loss: 2.0791\n",
            "Epoch [204/500], Loss: 2.0791\n",
            "Epoch [205/500], Loss: 2.0791\n",
            "Epoch [206/500], Loss: 2.0791\n",
            "Epoch [207/500], Loss: 2.0791\n",
            "Epoch [208/500], Loss: 2.0791\n",
            "Epoch [209/500], Loss: 2.0791\n",
            "Epoch [210/500], Loss: 2.0791\n",
            "Epoch [211/500], Loss: 2.0791\n",
            "Epoch [212/500], Loss: 2.0791\n",
            "Epoch [213/500], Loss: 2.0791\n",
            "Epoch [214/500], Loss: 2.0791\n",
            "Epoch [215/500], Loss: 2.0791\n",
            "Epoch [216/500], Loss: 2.0791\n",
            "Epoch [217/500], Loss: 2.0791\n",
            "Epoch [218/500], Loss: 2.0791\n",
            "Epoch [219/500], Loss: 2.0791\n",
            "Epoch [220/500], Loss: 2.0791\n",
            "Epoch [221/500], Loss: 2.0791\n",
            "Epoch [222/500], Loss: 2.0791\n",
            "Epoch [223/500], Loss: 2.0791\n",
            "Epoch [224/500], Loss: 2.0791\n",
            "Epoch [225/500], Loss: 2.0791\n",
            "Epoch [226/500], Loss: 2.0791\n",
            "Epoch [227/500], Loss: 2.0791\n",
            "Epoch [228/500], Loss: 2.0791\n",
            "Epoch [229/500], Loss: 2.0791\n",
            "Epoch [230/500], Loss: 2.0791\n",
            "Epoch [231/500], Loss: 2.0791\n",
            "Epoch [232/500], Loss: 2.0791\n",
            "Epoch [233/500], Loss: 2.0791\n",
            "Epoch [234/500], Loss: 2.0791\n",
            "Epoch [235/500], Loss: 2.0791\n",
            "Epoch [236/500], Loss: 2.0791\n",
            "Epoch [237/500], Loss: 2.0791\n",
            "Epoch [238/500], Loss: 2.0791\n",
            "Epoch [239/500], Loss: 2.0791\n",
            "Epoch [240/500], Loss: 2.0791\n",
            "Epoch [241/500], Loss: 2.0791\n",
            "Epoch [242/500], Loss: 2.0791\n",
            "Epoch [243/500], Loss: 2.0791\n",
            "Epoch [244/500], Loss: 2.0791\n",
            "Epoch [245/500], Loss: 2.0791\n",
            "Epoch [246/500], Loss: 2.0791\n",
            "Epoch [247/500], Loss: 2.0791\n",
            "Epoch [248/500], Loss: 2.0791\n",
            "Epoch [249/500], Loss: 2.0791\n",
            "Epoch [250/500], Loss: 2.0791\n",
            "Epoch [251/500], Loss: 2.0791\n",
            "Epoch [252/500], Loss: 2.0791\n",
            "Epoch [253/500], Loss: 2.0791\n",
            "Epoch [254/500], Loss: 2.0791\n",
            "Epoch [255/500], Loss: 2.0791\n",
            "Epoch [256/500], Loss: 2.0791\n",
            "Epoch [257/500], Loss: 2.0791\n",
            "Epoch [258/500], Loss: 2.0791\n",
            "Epoch [259/500], Loss: 2.0791\n",
            "Epoch [260/500], Loss: 2.0791\n",
            "Epoch [261/500], Loss: 2.0791\n",
            "Epoch [262/500], Loss: 2.0791\n",
            "Epoch [263/500], Loss: 2.0791\n",
            "Epoch [264/500], Loss: 2.0791\n",
            "Epoch [265/500], Loss: 2.0791\n",
            "Epoch [266/500], Loss: 2.0791\n",
            "Epoch [267/500], Loss: 2.0791\n",
            "Epoch [268/500], Loss: 2.0791\n",
            "Epoch [269/500], Loss: 2.0791\n",
            "Epoch [270/500], Loss: 2.0791\n",
            "Epoch [271/500], Loss: 2.0791\n",
            "Epoch [272/500], Loss: 2.0791\n",
            "Epoch [273/500], Loss: 2.0791\n",
            "Epoch [274/500], Loss: 2.0791\n",
            "Epoch [275/500], Loss: 2.0791\n",
            "Epoch [276/500], Loss: 2.0791\n",
            "Epoch [277/500], Loss: 2.0791\n",
            "Epoch [278/500], Loss: 2.0791\n",
            "Epoch [279/500], Loss: 2.0791\n",
            "Epoch [280/500], Loss: 2.0791\n",
            "Epoch [281/500], Loss: 2.0791\n",
            "Epoch [282/500], Loss: 2.0791\n",
            "Epoch [283/500], Loss: 2.0791\n",
            "Epoch [284/500], Loss: 2.0791\n",
            "Epoch [285/500], Loss: 2.0791\n",
            "Epoch [286/500], Loss: 2.0791\n",
            "Epoch [287/500], Loss: 2.0791\n",
            "Epoch [288/500], Loss: 2.0791\n",
            "Epoch [289/500], Loss: 2.0791\n",
            "Epoch [290/500], Loss: 2.0791\n",
            "Epoch [291/500], Loss: 2.0791\n",
            "Epoch [292/500], Loss: 2.0791\n",
            "Epoch [293/500], Loss: 2.0791\n",
            "Epoch [294/500], Loss: 2.0791\n",
            "Epoch [295/500], Loss: 2.0791\n",
            "Epoch [296/500], Loss: 2.0791\n",
            "Epoch [297/500], Loss: 2.0791\n",
            "Epoch [298/500], Loss: 2.0791\n",
            "Epoch [299/500], Loss: 2.0791\n",
            "Epoch [300/500], Loss: 2.0791\n",
            "Epoch [301/500], Loss: 2.0791\n",
            "Epoch [302/500], Loss: 2.0791\n",
            "Epoch [303/500], Loss: 2.0791\n",
            "Epoch [304/500], Loss: 2.0791\n",
            "Epoch [305/500], Loss: 2.0791\n",
            "Epoch [306/500], Loss: 2.0791\n",
            "Epoch [307/500], Loss: 2.0791\n",
            "Epoch [308/500], Loss: 2.0791\n",
            "Epoch [309/500], Loss: 2.0791\n",
            "Epoch [310/500], Loss: 2.0791\n",
            "Epoch [311/500], Loss: 2.0791\n",
            "Epoch [312/500], Loss: 2.0791\n",
            "Epoch [313/500], Loss: 2.0791\n",
            "Epoch [314/500], Loss: 2.0791\n",
            "Epoch [315/500], Loss: 2.0791\n",
            "Epoch [316/500], Loss: 2.0791\n",
            "Epoch [317/500], Loss: 2.0791\n",
            "Epoch [318/500], Loss: 2.0791\n",
            "Epoch [319/500], Loss: 2.0791\n",
            "Epoch [320/500], Loss: 2.0791\n",
            "Epoch [321/500], Loss: 2.0791\n",
            "Epoch [322/500], Loss: 2.0791\n",
            "Epoch [323/500], Loss: 2.0791\n",
            "Epoch [324/500], Loss: 2.0791\n",
            "Epoch [325/500], Loss: 2.0791\n",
            "Epoch [326/500], Loss: 2.0791\n",
            "Epoch [327/500], Loss: 2.0791\n",
            "Epoch [328/500], Loss: 2.0791\n",
            "Epoch [329/500], Loss: 2.0791\n",
            "Epoch [330/500], Loss: 2.0791\n",
            "Epoch [331/500], Loss: 2.0791\n",
            "Epoch [332/500], Loss: 2.0791\n",
            "Epoch [333/500], Loss: 2.0791\n",
            "Epoch [334/500], Loss: 2.0791\n",
            "Epoch [335/500], Loss: 2.0791\n",
            "Epoch [336/500], Loss: 2.0791\n",
            "Epoch [337/500], Loss: 2.0791\n",
            "Epoch [338/500], Loss: 2.0791\n",
            "Epoch [339/500], Loss: 2.0791\n",
            "Epoch [340/500], Loss: 2.0791\n",
            "Epoch [341/500], Loss: 2.0791\n",
            "Epoch [342/500], Loss: 2.0791\n",
            "Epoch [343/500], Loss: 2.0791\n",
            "Epoch [344/500], Loss: 2.0791\n",
            "Epoch [345/500], Loss: 2.0791\n",
            "Epoch [346/500], Loss: 2.0791\n",
            "Epoch [347/500], Loss: 2.0791\n",
            "Epoch [348/500], Loss: 2.0791\n",
            "Epoch [349/500], Loss: 2.0791\n",
            "Epoch [350/500], Loss: 2.0791\n",
            "Epoch [351/500], Loss: 2.0791\n",
            "Epoch [352/500], Loss: 2.0791\n",
            "Epoch [353/500], Loss: 2.0791\n",
            "Epoch [354/500], Loss: 2.0791\n",
            "Epoch [355/500], Loss: 2.0791\n",
            "Epoch [356/500], Loss: 2.0791\n",
            "Epoch [357/500], Loss: 2.0791\n",
            "Epoch [358/500], Loss: 2.0791\n",
            "Epoch [359/500], Loss: 2.0791\n",
            "Epoch [360/500], Loss: 2.0791\n",
            "Epoch [361/500], Loss: 2.0791\n",
            "Epoch [362/500], Loss: 2.0791\n",
            "Epoch [363/500], Loss: 2.0791\n",
            "Epoch [364/500], Loss: 2.0791\n",
            "Epoch [365/500], Loss: 2.0791\n",
            "Epoch [366/500], Loss: 2.0791\n",
            "Epoch [367/500], Loss: 2.0791\n",
            "Epoch [368/500], Loss: 2.0791\n",
            "Epoch [369/500], Loss: 2.0791\n",
            "Epoch [370/500], Loss: 2.0791\n",
            "Epoch [371/500], Loss: 2.0791\n",
            "Epoch [372/500], Loss: 2.0791\n",
            "Epoch [373/500], Loss: 2.0791\n",
            "Epoch [374/500], Loss: 2.0791\n",
            "Epoch [375/500], Loss: 2.0791\n",
            "Epoch [376/500], Loss: 2.0791\n",
            "Epoch [377/500], Loss: 2.0791\n",
            "Epoch [378/500], Loss: 2.0791\n",
            "Epoch [379/500], Loss: 2.0791\n",
            "Epoch [380/500], Loss: 2.0791\n",
            "Epoch [381/500], Loss: 2.0791\n",
            "Epoch [382/500], Loss: 2.0791\n",
            "Epoch [383/500], Loss: 2.0791\n",
            "Epoch [384/500], Loss: 2.0791\n",
            "Epoch [385/500], Loss: 2.0791\n",
            "Epoch [386/500], Loss: 2.0791\n",
            "Epoch [387/500], Loss: 2.0791\n",
            "Epoch [388/500], Loss: 2.0791\n",
            "Epoch [389/500], Loss: 2.0791\n",
            "Epoch [390/500], Loss: 2.0791\n",
            "Epoch [391/500], Loss: 2.0791\n",
            "Epoch [392/500], Loss: 2.0791\n",
            "Epoch [393/500], Loss: 2.0791\n",
            "Epoch [394/500], Loss: 2.0791\n",
            "Epoch [395/500], Loss: 2.0791\n",
            "Epoch [396/500], Loss: 2.0791\n",
            "Epoch [397/500], Loss: 2.0791\n",
            "Epoch [398/500], Loss: 2.0791\n",
            "Epoch [399/500], Loss: 2.0791\n",
            "Epoch [400/500], Loss: 2.0791\n",
            "Epoch [401/500], Loss: 2.0791\n",
            "Epoch [402/500], Loss: 2.0791\n",
            "Epoch [403/500], Loss: 2.0791\n",
            "Epoch [404/500], Loss: 2.0791\n",
            "Epoch [405/500], Loss: 2.0791\n",
            "Epoch [406/500], Loss: 2.0791\n",
            "Epoch [407/500], Loss: 2.0791\n",
            "Epoch [408/500], Loss: 2.0791\n",
            "Epoch [409/500], Loss: 2.0791\n",
            "Epoch [410/500], Loss: 2.0791\n",
            "Epoch [411/500], Loss: 2.0791\n",
            "Epoch [412/500], Loss: 2.0791\n",
            "Epoch [413/500], Loss: 2.0791\n",
            "Epoch [414/500], Loss: 2.0791\n",
            "Epoch [415/500], Loss: 2.0791\n",
            "Epoch [416/500], Loss: 2.0791\n",
            "Epoch [417/500], Loss: 2.0791\n",
            "Epoch [418/500], Loss: 2.0791\n",
            "Epoch [419/500], Loss: 2.0791\n",
            "Epoch [420/500], Loss: 2.0791\n",
            "Epoch [421/500], Loss: 2.0791\n",
            "Epoch [422/500], Loss: 2.0791\n",
            "Epoch [423/500], Loss: 2.0791\n",
            "Epoch [424/500], Loss: 2.0791\n",
            "Epoch [425/500], Loss: 2.0791\n",
            "Epoch [426/500], Loss: 2.0791\n",
            "Epoch [427/500], Loss: 2.0791\n",
            "Epoch [428/500], Loss: 2.0791\n",
            "Epoch [429/500], Loss: 2.0791\n",
            "Epoch [430/500], Loss: 2.0791\n",
            "Epoch [431/500], Loss: 2.0791\n",
            "Epoch [432/500], Loss: 2.0791\n",
            "Epoch [433/500], Loss: 2.0791\n",
            "Epoch [434/500], Loss: 2.0791\n",
            "Epoch [435/500], Loss: 2.0791\n",
            "Epoch [436/500], Loss: 2.0791\n",
            "Epoch [437/500], Loss: 2.0791\n",
            "Epoch [438/500], Loss: 2.0791\n",
            "Epoch [439/500], Loss: 2.0791\n",
            "Epoch [440/500], Loss: 2.0791\n",
            "Epoch [441/500], Loss: 2.0791\n",
            "Epoch [442/500], Loss: 2.0791\n",
            "Epoch [443/500], Loss: 2.0791\n",
            "Epoch [444/500], Loss: 2.0791\n",
            "Epoch [445/500], Loss: 2.0791\n",
            "Epoch [446/500], Loss: 2.0791\n",
            "Epoch [447/500], Loss: 2.0791\n",
            "Epoch [448/500], Loss: 2.0791\n",
            "Epoch [449/500], Loss: 2.0791\n",
            "Epoch [450/500], Loss: 2.0791\n",
            "Epoch [451/500], Loss: 2.0791\n",
            "Epoch [452/500], Loss: 2.0791\n",
            "Epoch [453/500], Loss: 2.0791\n",
            "Epoch [454/500], Loss: 2.0791\n",
            "Epoch [455/500], Loss: 2.0791\n",
            "Epoch [456/500], Loss: 2.0791\n",
            "Epoch [457/500], Loss: 2.0791\n",
            "Epoch [458/500], Loss: 2.0791\n",
            "Epoch [459/500], Loss: 2.0791\n",
            "Epoch [460/500], Loss: 2.0791\n",
            "Epoch [461/500], Loss: 2.0791\n",
            "Epoch [462/500], Loss: 2.0791\n",
            "Epoch [463/500], Loss: 2.0791\n",
            "Epoch [464/500], Loss: 2.0791\n",
            "Epoch [465/500], Loss: 2.0791\n",
            "Epoch [466/500], Loss: 2.0791\n",
            "Epoch [467/500], Loss: 2.0791\n",
            "Epoch [468/500], Loss: 2.0791\n",
            "Epoch [469/500], Loss: 2.0791\n",
            "Epoch [470/500], Loss: 2.0791\n",
            "Epoch [471/500], Loss: 2.0791\n",
            "Epoch [472/500], Loss: 2.0791\n",
            "Epoch [473/500], Loss: 2.0791\n",
            "Epoch [474/500], Loss: 2.0791\n",
            "Epoch [475/500], Loss: 2.0791\n",
            "Epoch [476/500], Loss: 2.0791\n",
            "Epoch [477/500], Loss: 2.0791\n",
            "Epoch [478/500], Loss: 2.0791\n",
            "Epoch [479/500], Loss: 2.0791\n",
            "Epoch [480/500], Loss: 2.0791\n",
            "Epoch [481/500], Loss: 2.0791\n",
            "Epoch [482/500], Loss: 2.0791\n",
            "Epoch [483/500], Loss: 2.0791\n",
            "Epoch [484/500], Loss: 2.0791\n",
            "Epoch [485/500], Loss: 2.0791\n",
            "Epoch [486/500], Loss: 2.0791\n",
            "Epoch [487/500], Loss: 2.0791\n",
            "Epoch [488/500], Loss: 2.0791\n",
            "Epoch [489/500], Loss: 2.0791\n",
            "Epoch [490/500], Loss: 2.0791\n",
            "Epoch [491/500], Loss: 2.0791\n",
            "Epoch [492/500], Loss: 2.0791\n",
            "Epoch [493/500], Loss: 2.0791\n",
            "Epoch [494/500], Loss: 2.0791\n",
            "Epoch [495/500], Loss: 2.0791\n",
            "Epoch [496/500], Loss: 2.0791\n",
            "Epoch [497/500], Loss: 2.0791\n",
            "Epoch [498/500], Loss: 2.0791\n",
            "Epoch [499/500], Loss: 2.0791\n",
            "Epoch [500/500], Loss: 2.0791\n",
            "Accuracy: 0.1429\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MVmS_UhT-Y71"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}